<!-- <section id="contact" class="container contact-section text-center"> -->

<section id="abstract" class="content-section">
  <div class="container about-section">
    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        <h2 class="text-center">Abstracts</h2>
        <div class="col-lg-8 col-lg-offset-2">

          
          <h4> Yao Xie </h4>
          <p> Hypothesis tests via distributionally robust optimization</p>
          We consider a general data-driven robust hypothesis test formulation to find an optimal test (function) that minimizes the worst-case performance regarding distributions that are close to the empirical distributions with respect to some divergence, in particular, the Wasserstein and the sink horn divergences. The robust tests are beneficial, for instance, for cases with limited or unbalanced samples - such a scenario often arises from applications such as health care, online change-point detection, and anomaly detection. We present a distributionally robust optimization framework to solve such a problem and study the computational and statistical properties of the proposed test by presenting a tractable convex reformulation of the original infinite-dimensional variational problem. Finally, I will present the generalization of the approach to other related problems, including domain adaptation. 
          <br><br>

          <h4>Samory Kpotufe</h4>
          <p> Tracking Most Significant Arm Switches in Bandits </p>
           In bandit with distribution shifts, one aims to automatically adapt to unknown changes in reward distribution, and restart exploration when necessary. While this problem has received attention for many years, no adaptive procedure was known till a recent breakthrough of Auer et al (2018, 2019) which guarantees an optimal (dynamic) regret (LT)^{1/2}, for T rounds and L stationary phases. 
           <br><br>
           However, while this rate is tight in the worst case, it leaves open whether faster rates are possible, adaptively, if few changes in distribution are actually severe, e.g., involve no change in best arm. We provide a positive answer, showing that in fact, a much weaker notion of change can be adapted to, which can yield significantly faster rates than previously known, whether as expressed in terms of number of best arm switches--for which no adaptive procedure was known, or in terms of total variation. Finally, our parametrization captures at once, both stochastic and non-stochastic adversarial settings. 
          <br><br><br>
          
          
          <h3>Ludwig Schmidt</h3>
          <h4> A data-centric view on robustness</h4>
          <p>Over the past few years, researchers have proposed many ways to measure the robustness of machine learning models. In the first part of the talk, we will survey the current robustness landscape based on a large-scale experimental study involving more than 200 different models and test conditions. Despite the large variety of test conditions, common trends emerge: (i) robustness to natural distribution shift and synthetic perturbations are distinct phenomena, (ii) current algorithmic techniques have little effect on robustness to natural distribution shifts, (iii) training on more diverse datasets offers robustness gains on several natural distribution shifts.
          </p><br><br>
          <p>In the second part of the talk, we then leverage the aforementioned insights to improve OpenAIâ€™s CLIP model. CLIP achieved unprecedented robustness on several natural distribution shifts, but only when used as a zero-shot model. The zero-shot evaluation precludes the use of extra data for fine-tuning and hence leads to lower performance when there is a specific task of interest. To address this issue, we introduce a simple yet effective method for fine-tuning zero-shot models that leads to large robustness gains on several distribution shifts without reducing in-distribution performance.
          </p><br><br><br>
            
          <h4>Jamie Morgenstern</h4>
          <h5>Endogeneous distribution shifts in competitive environments</h5>
           <p>In this talk, I'll describe recent work exploring the dynamics between ML systems and the populations they serve, particularly when the models deployed impact what populations a system will have as future customers. 
          </p><br><br>
   
        </div>
      </div>
<!--     ### The following lines are not required for the last tab -->
      <div class="text-center">
        <a href="#contact" class="btn btn-circle page-scroll">
          <i class="fa fa-angle-double-down animated"></i>
        </a>
      </div>
    </div>
</section>
